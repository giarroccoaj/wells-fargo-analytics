{"name":"Wells Fargo Analytics Competition","tagline":"","body":"# Introduction.\r\n \r\nIntroduction\r\n\r\nThroughout my Data 101 course I was introduced to the concepts and theories behind Data Science that make it such an interdisciplinary field. Using the R programming language and popular algorithms in Big Data I was able to explore real world data and gain practical experience as a Data Scientist. \r\n\r\nA recurring theme, or observation, when looking into Big Data is the idea that large data sets are difficult to make sense of (i.e: unstructured). The challenge is often to make sense of (structure) this data. \r\n\r\nWells Fargo opened a competition that I participated in for my final project in my Data 101 course. \r\n\r\n\r\n# The Competition.\r\nThe Competition\r\n\r\n“Dialogues on social media can provide tremendous insight into the behaviors, desires, pains, and thoughts of consumers. We'd like your help in developing a repeatable process that identifies, classifies, and extracts the underlying drivers of consumer financial conversations and comments in social media data.”\r\n\r\n\r\nUsing just [this document](https://d18qs7yq39787j.cloudfront.net/uploads/contestfile/93/8af8575b213c-2015%2BWells%2BFargo%2BCampus%2BAnalytic%2BChallenge%2BDataset.txt), a team of 3 others and myself were tasked with answering two questions: \r\n\r\n**Question #1: What financial topics do consumers discuss on social media and what caused the consumers to post about this topic? **\r\n\r\n**Question #2: Are the topics and “substance” consistent across the industry or are they isolated to individual banks? **\r\n\r\nWith these two questions in mind, we had our work cut out for us.\r\n\r\n\r\n\r\n# The Solution\r\n\r\nUsing the dataset, the first challenge – like in much of big data – was to structure or clean its contents. Un-cleaned, the social media posts looked like this:\r\n\r\n![Note the recurrent terms, i.e: \"INTERNET\", \"twit_handl\"... We need to take these out!](http://i.imgur.com/8hdjJ2H.png)\r\n\r\n\r\nWith over 200,000 posts in the entire data set, our team chose make a smaller data frame of 10,000 random samples - what we believed to be a large enough sample size. \r\n\r\n```R\r\ndf = read.table('dataset.txt',sep=\"|\",header=T) #convert .txt document to data frame\r\nidx.10000 = sample(1:nrow(df),10000) #find 10,000 random numbers\r\ndf.10000 = df[idx.10000,] #create new data frame of 10,000 posts using random indices\r\n```\r\n\r\nIn an effort to clean the data frame, it must be made suitable for the text mining package and therefore it must be a Corpus(collection of text documents). Here is the code to convert Data Frame to Corpus and code to clean Corpus of unnecessary meta data.\r\n\r\n```R\r\nlibrary(tm) \r\ndocs <- Corpus(DataframeSource(as.data.frame(df.10000[,6]))) \r\nlibrary(rJava)\r\ndocs <- tm_map(docs, content_transformer(tolower)) # convert to lowercase first\r\n#metaData/recurrent words\r\nmyMeta <- c(\"name\",\"bank\",\"banka\",\"bankb\",\"bankc\", \"bankd\", \"banke\",\r\n            \"internet\",\"https\", \"twit_hndl\", \"twit_hndl_banka\",\r\n            \"twit_hndl_bankb\",\"twit_hndl_bankc\",\"twit_hndl_bankd\", \"phone\",\r\n            \"dirmsg\", \"street\",\"name_resp\",\"bankds\", \"and\", \"for\", \"the\", \"you\",\r\n            \"twithndlbanka\",\"dirmsg\",\"ret_twit\")\r\n#REMEMBER: IT MATTERS THE ORDER IN WHICH TM_MAP EXPRESSIONS ARE RUN\r\ndocs <- tm_map(docs, removeWords, myMeta) #remove words found in myMeta vector\r\ndocs <- tm_map(docs, removeWords, stopwords('english')) #remove english stopwords\r\ndocs <- tm_map(docs, removeWords, stopwords(kind = \"SMART\")) #remove even more stopwords\r\ndocs <- tm_map(docs, stripWhitespace)\r\ndocs <- tm_map(docs, removePunctuation)\r\ndocs <- tm_map(docs, PlainTextDocument)\r\n```\r\n\r\n\r\nFollowing this code, the posts now look like the following\r\n\r\n\r\n![](http://i.imgur.com/dWYC0Wo.png)\r\n\r\n \r\n\r\nNow that the data has been cleaned, analysis can be done.\r\n\r\nTo find out what topics consumers talked about on social media, we went ahead and found the most frequent terms using 10,000 posts.\r\n\r\nBut to do this, the Corpus must first be turned into a Document-Term Matrix\r\n\r\n```R\r\ndtm <- DocumentTermMatrix(docs)\r\ndtm = removeSparseTerms(dtm, 0.98) #removes words with very low frequency\r\nfindFreqTerms(dtm,100) #finds words that occur at least 100x\r\n```\r\n\r\n\r\n\r\n### Creating pages manually\r\nIf you prefer to not use the automatic generator, push a branch named `gh-pages` to your repository to create a page manually. In addition to supporting regular HTML content, GitHub Pages support Jekyll, a simple, blog aware static site generator. Jekyll makes it easy to create site-wide headers and footers without having to copy them across every page. It also offers intelligent blog support and other advanced templating features.\r\n\r\n### Authors and Contributors\r\nYou can @mention a GitHub username to generate a link to their profile. The resulting `<a>` element will link to the contributor’s GitHub Profile. For example: In 2007, Chris Wanstrath (@defunkt), PJ Hyett (@pjhyett), and Tom Preston-Werner (@mojombo) founded GitHub.\r\n\r\n### Support or Contact\r\nHaving trouble with Pages? Check out our [documentation](https://help.github.com/pages) or [contact support](https://github.com/contact) and we’ll help you sort it out.","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}