{"name":"Wells Fargo Analytics Competition","tagline":"","body":"# Introduction.\r\n \r\nIntroduction\r\n\r\nThroughout my Data 101 course I was introduced to the concepts and theories behind Data Science that make it such an interdisciplinary field. Using the R programming language and popular algorithms in Big Data I was able to explore real world data and gain practical experience as a Data Scientist. \r\n\r\nA recurring theme, or observation, when looking into Big Data is the idea that large data sets are difficult to make sense of (i.e: unstructured). The challenge is often to make sense of (structure) this data. \r\n\r\nWells Fargo opened a competition that I participated in for my final project in my Data 101 course. \r\n\r\n\r\n# The Competition.\r\nThe Competition\r\n\r\n“Dialogues on social media can provide tremendous insight into the behaviors, desires, pains, and thoughts of consumers. We'd like your help in developing a repeatable process that identifies, classifies, and extracts the underlying drivers of consumer financial conversations and comments in social media data.”\r\n\r\n\r\nUsing just [this document](https://d18qs7yq39787j.cloudfront.net/uploads/contestfile/93/8af8575b213c-2015%2BWells%2BFargo%2BCampus%2BAnalytic%2BChallenge%2BDataset.txt), a team of 3 others and myself were tasked with answering two questions: \r\n\r\nQuestion #1: What financial topics do consumers discuss on social media and what caused the consumers to post about this topic? \r\n\r\nQuestion #2: Are the topics and “substance” consistent across the industry or are they isolated to individual banks? \r\n\r\n\r\nWith these two questions in mind, we had our work cut out for us.\r\n\r\n\r\n\r\n# The Solution\r\n\r\nUsing the dataset, the first challenge – like in much of big data – was to structure or clean its contents. Un-cleaned, the social media posts looked like this:\r\n\r\n\r\n\r\n \r\n\r\n \r\nAs you can see there are many of the same items that pop up post after post. Some of them being “twit_hndl”, “BankD”, “INTERNET”, “Name”, etc. \r\n\r\nUsing a sample size of 10,000 posts, we stripped out words or topics (such as “twit_hndl”, “INTERNET”) that we believed would not lend any importance to subsequent analysis. \r\n\r\nIn an effort to clean, the data frame of 10,000 posts must be made suitable for the text mining package and therefore it must be a Corpus(collection of text documents). Here is the code to convert Data Frame to Corpus and to clean Corpus of unnecessary meta data.\r\n'''R\r\nlibrary(tm) \r\ndocs <- Corpus(DataframeSource(as.data.frame(df[,6])))\r\n#REMEMBER: IT MATTERS THE ORDER IN WHICH TM_MAP EXPRESSIONS ARE RUN\r\nlibrary(rJava)\r\ndocs <- tm_map(docs, content_transformer(tolower)) # convert to lowercase first\r\nmyMeta <- c(\"name\",\"bank\",\"banka\",\"bankb\",\"bankc\", \"bankd\", \"banke\",\r\n            \"internet\",\"https\", \"twit_hndl\", \"twit_hndl_banka\",\r\n            \"twit_hndl_bankb\",\"twit_hndl_bankc\",\"twit_hndl_bankd\", \"phone\",\r\n            \"dirmsg\", \"street\",\"name_resp\",\"bankds\", \"and\", \"for\", \"the\", \"you\",\r\n            \"twithndlbanka\",\"dirmsg\",\"ret_twit\")\r\ndocs <- tm_map(docs, removeWords, myMeta)\r\ndocs <- tm_map(docs, removeWords, stopwords('english'))\r\ndocs <- tm_map(docs, removeWords, stopwords(kind = \"SMART\"))\r\n#metaData/recurrent words\r\n\r\ndocs <- tm_map(docs, stripWhitespace)\r\ndocs <- tm_map(docs, removePunctuation)\r\ndocs <- tm_map(docs, PlainTextDocument)\r\n-> '''\r\n\r\n\r\nFollowing this code, the posts now look like the following\r\n\r\n \r\n\r\nNow that the data has been cleaned, analysis can follow.\r\n\r\nTo find out what topics consumers talked about on social media, we went ahead and found the most frequent terms using 10,000 posts.\r\n\r\nThe Corpus must first be turned into a Document-Term Matrix\r\n'''R\r\n\r\ndtm <- DocumentTermMatrix(docs)\r\ndtm = removeSparseTerms(dtm, 0.98) #removes words with very low frequency\r\nfindFreqTerms(dtm,100) #finds words that occur at least 100x\r\n-> '''\r\n\r\n\r\n\r\n### Creating pages manually\r\nIf you prefer to not use the automatic generator, push a branch named `gh-pages` to your repository to create a page manually. In addition to supporting regular HTML content, GitHub Pages support Jekyll, a simple, blog aware static site generator. Jekyll makes it easy to create site-wide headers and footers without having to copy them across every page. It also offers intelligent blog support and other advanced templating features.\r\n\r\n### Authors and Contributors\r\nYou can @mention a GitHub username to generate a link to their profile. The resulting `<a>` element will link to the contributor’s GitHub Profile. For example: In 2007, Chris Wanstrath (@defunkt), PJ Hyett (@pjhyett), and Tom Preston-Werner (@mojombo) founded GitHub.\r\n\r\n### Support or Contact\r\nHaving trouble with Pages? Check out our [documentation](https://help.github.com/pages) or [contact support](https://github.com/contact) and we’ll help you sort it out.\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}