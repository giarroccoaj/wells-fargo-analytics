{"name":"Wells Fargo Analytics Competition","tagline":"","body":"# Introduction.\r\n\r\nThroughout my Data 101 course I was introduced to the concepts and theories behind Data Science that make it such an interdisciplinary field. Using the R programming language and popular algorithms in Big Data I was able to explore real world data and gain practical experience as a Data Scientist. \r\n\r\nA recurring theme, or observation, when looking into Big Data is the idea that large data sets are difficult to make sense of (i.e: unstructured). The challenge is often to make sense of (structure) this data. \r\n\r\nWells Fargo opened a competition that I participated in for my final project in my Data 101 course. \r\n\r\n\r\n# The Competition.\r\n\r\n“Dialogues on social media can provide tremendous insight into the behaviors, desires, pains, and thoughts of consumers. We'd like your help in developing a repeatable process that identifies, classifies, and extracts the underlying drivers of consumer financial conversations and comments in social media data.”\r\n\r\n\r\nUsing just [this document](https://d18qs7yq39787j.cloudfront.net/uploads/contestfile/93/8af8575b213c-2015%2BWells%2BFargo%2BCampus%2BAnalytic%2BChallenge%2BDataset.txt), a team of 3 others and myself were tasked with answering two questions: \r\n\r\n**Question #1: What financial topics do consumers discuss on social media and what caused the consumers to post about this topic? **\r\n\r\n**Question #2: Are the topics and “substance” consistent across the industry or are they isolated to individual banks? **\r\n\r\nWith these two questions in mind, we had our work cut out for us.\r\n\r\n\r\n\r\n# The Solution\r\n\r\nUsing the dataset, the first challenge – like in much of big data – was to structure or clean its contents. Un-cleaned, the social media posts looked like this:\r\n\r\n![Note the recurrent terms, i.e: \"INTERNET\", \"twit_handl\"... We need to take these out!](http://i.imgur.com/8hdjJ2H.png)\r\n\r\n\r\nWith over 200,000 posts in the entire data set, our team chose make a smaller data frame of 10,000 random samples - what we believed to be a large enough sample size. \r\n\r\n```R\r\ndf = read.table('dataset.txt',sep=\"|\",header=T) #convert .txt document to data frame\r\nidx.10000 = sample(1:nrow(df),10000) #find 10,000 random numbers\r\ndf.10000 = df[idx.10000,] #create new data frame of 10,000 posts using random indices\r\n```\r\n\r\nIn an effort to clean the data frame, it must be made suitable for the text mining package and therefore it must be a Corpus(collection of text documents). Here is the code to convert Data Frame to Corpus and code to clean Corpus of unnecessary meta data.\r\n\r\n```R\r\nlibrary(tm) \r\ndocs <- Corpus(DataframeSource(as.data.frame(df.10000[,6]))) \r\nlibrary(rJava)\r\ndocs <- tm_map(docs, content_transformer(tolower)) # convert to lowercase first\r\n#metaData/recurrent words\r\nmyMeta <- c(\"name\",\"bank\",\"banka\",\"bankb\",\"bankc\", \"bankd\", \"banke\",\r\n            \"internet\",\"https\", \"twit_hndl\", \"twit_hndl_banka\",\r\n            \"twit_hndl_bankb\",\"twit_hndl_bankc\",\"twit_hndl_bankd\", \"phone\",\r\n            \"dirmsg\", \"street\",\"name_resp\",\"bankds\", \"and\", \"for\", \"the\", \"you\",\r\n            \"twithndlbanka\",\"dirmsg\",\"ret_twit\")\r\n#REMEMBER: IT MATTERS THE ORDER IN WHICH TM_MAP EXPRESSIONS ARE RUN\r\ndocs <- tm_map(docs, removeWords, myMeta) #remove words found in myMeta vector\r\ndocs <- tm_map(docs, removeWords, stopwords('english')) #remove english stopwords\r\ndocs <- tm_map(docs, removeWords, stopwords(kind = \"SMART\")) #remove even more stopwords\r\ndocs <- tm_map(docs, stripWhitespace)\r\ndocs <- tm_map(docs, removePunctuation)\r\ndocs <- tm_map(docs, PlainTextDocument)\r\n```\r\n\r\n\r\nFollowing this code, the posts now look like the following\r\n\r\n\r\n![](http://i.imgur.com/dWYC0Wo.png)\r\n\r\n \r\n\r\nNow that the data has been cleaned, analysis can be done.\r\n\r\nTo find out what topics consumers talked about on social media, we went ahead ran sentiment analysis on the newly cleaned data. Using our sample of 10,000 social media posts, we compared them to lists of positive and negative words. We also ran sentiment analysis for each bank talked about in the posts. Here is code to subset each bank into its own corpus.\r\n\r\n```R\r\nbankA.idx = which(sapply(df$FullText,function(x) grepl(\"BankA\",x))) #searches for posts containg \"BankA\"\r\nbankB.idx = which(sapply(df$FullText,function(x) grepl(\"BankB\",x)))\r\nbankC.idx = which(sapply(df$FullText,function(x) grepl(\"BankC\",x)))\r\nbankD.idx = which(sapply(df$FullText,function(x) grepl(\"BankD\",x)))\r\n\r\ndf$BankID = vector(mode=\"numeric\",length = nrow(df))\r\ndf$BankID[bankA.idx] = \"BankA\" #sets new column in data frame to ID each post mentioning BankA\r\ndf$BankID[bankB.idx] = \"BankB\"\r\ndf$BankID[bankC.idx] = \"BankC\"\r\ndf$BankID[bankD.idx] = \"BankD\"\r\n\r\nbankA.docs = docs[bankA.idx] #All posts mentioning bankA\r\nbankB.docs = docs[bankB.idx]\r\nbankC.docs = docs[bankC.idx]\r\nbankD.docs = docs[bankD.idx] \r\n\r\n```\r\nFor sentiment analysis to work, the Corpus must first be turned into a Document-Term Matrix, removed of its sparse terms, and turned back into a data frame.\r\n\r\n```R\r\ndtm <- DocumentTermMatrix(docs)\r\ndtm = removeSparseTerms(dtm, 0.98) #removes words with very low frequency\r\nnew.df <- data.frame(text=unlist(sapply(docs, `[[`, \"content\")), stringsAsFactors=FALSE)\r\n```\r\n\r\nWith new.df, we come up with sentiment scores that can be used to make wordclouds and other plots using the following function\r\n\r\n```R\r\npos <- scan('positive-words.txt',what='character',comment.char=';')\r\nneg <- scan('negative-words.txt',what='character',comment.char=';')\r\n\r\nscore.sentiment = function(sentences, pos.words, neg.words, .progress='none')\r\n{\r\n  require(plyr)\r\n  require(stringr)\r\n  \r\n  # we got a vector of sentences. plyr will handle a list\r\n  # or a vector as an \"l\" for us\r\n  # we want a simple array (\"a\") of scores back, so we use \r\n  # \"l\" + \"a\" + \"ply\" = \"laply\":\r\n  scores = laply(sentences, function(sentence, pos.words, neg.words) {\r\n    \r\n    # clean up sentences with R's regex-driven global substitute, gsub():\r\n    sentence = gsub('[[:punct:]]', '', sentence)\r\n    sentence = gsub('[[:cntrl:]]', '', sentence)\r\n    sentence = gsub('\\\\d+', '', sentence)\r\n    # and convert to lower case:\r\n    sentence = tolower(sentence)\r\n    \r\n    # split into words. str_split is in the stringr package\r\n    word.list = str_split(sentence, '\\\\s+')\r\n    # sometimes a list() is one level of hierarchy too much\r\n    words = unlist(word.list)\r\n    \r\n    # compare our words to the dictionaries of positive & negative terms\r\n    pos.matches = match(words, pos.words)\r\n    neg.matches = match(words, neg.words)\r\n    \r\n    # match() returns the position of the matched term or NA\r\n    # we just want a TRUE/FALSE:\r\n    pos.matches = !is.na(pos.matches)\r\n    neg.matches = !is.na(neg.matches)\r\n    \r\n    # and conveniently enough, TRUE/FALSE will be treated as 1/0 by sum():\r\n    score = sum(pos.matches) - sum(neg.matches)\r\n    \r\n    return(score)\r\n  }, pos.words, neg.words, .progress=.progress )\r\n  \r\n  scores.df = data.frame(score=scores, text=sentences)\r\n  return(scores.df)\r\n}\r\n\r\nscores = score.sentiment(new.df$FullText, pos, neg, .progress='text')\r\n```\r\n\r\nFor the data frame containing 10,000 we came up with the following positive and negative wordclouds...\r\n\r\n![Positive Sentiment](http://i.imgur.com/hBcgAz8.png)\r\n\r\n![Negative Sentiment](http://i.imgur.com/ToEVUrM.png)\r\n\r\nThe positive and negative word clouds speak for themselves. With the increasing number of people utilizing the remote call centers, it is increasingly important for those people in the centers to be properly trained and held to the highest standard for their customer service. Banks should consider expanding their budgets for training those individuals to ensure the best customer experience and retain current clientele. That's the conclusion that can be made from these word clouds.\r\n\r\n![Average Sentiment Score](http://i.imgur.com/50MIJeJ.png)\r\n\r\nLooking more deeply into the sentiment, the  plot above shows that people are more likely to leave positive posts on Facebook rather than on Twitter. This conclusion could have further implications as to how banks market themselves on each social medium. While Twitter acts as a live feed allowing more real-time reviews, Facebook acts as a page. Therefore, Facebook may have more volunteer bias than Twitter. People may be more inclined to tweet after a bad experience than leave a review.\r\n\r\n\r\nAs for the financial topics being discussed via social media across individual banks, here are wordclouds \r\n\r\n\r\n\r\n\r\n Bank D’s data was the most helpful providing areas such as “financial management”, “grants”, “advisers” while the remaining banks had more broad terms such as “credit card”. This may due to a policy Bank D has where they encourage their customers to leave specific comments so when doing analysis like this, it is easier to have more solid takeaways.\r\n\r\n\r\nThe bank that stood out as being different in the word clusters was Bank D. While all other banks shared words “like” and “can,” Bank D showed more active words such as “swing,” “apply,” and “program.” Bank D may be more effective on a basic customer service level, therefore prompting tweets only when more complex issues come about. \r\n\r\n\r\n![](http://www.gliffy.com/go/publish/image/9640825/L.png)\r\n\r\n\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}