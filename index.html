<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Wells Fargo Analytics Competition by giarroccoaj</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Wells Fargo Analytics Competition</h1>
      <h2 class="project-tagline"></h2>
      <a href="https://github.com/giarroccoaj/wells-fargo-analytics" class="btn">View on GitHub</a>
      <a href="https://github.com/giarroccoaj/wells-fargo-analytics/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/giarroccoaj/wells-fargo-analytics/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <h1>
<a id="introduction" class="anchor" href="#introduction" aria-hidden="true"><span class="octicon octicon-link"></span></a>Introduction.</h1>

<p>Introduction</p>

<p>Throughout my Data 101 course I was introduced to the concepts and theories behind Data Science that make it such an interdisciplinary field. Using the R programming language and popular algorithms in Big Data I was able to explore real world data and gain practical experience as a Data Scientist. </p>

<p>A recurring theme, or observation, when looking into Big Data is the idea that large data sets are difficult to make sense of (i.e: unstructured). The challenge is often to make sense of (structure) this data. </p>

<p>Wells Fargo opened a competition that I participated in for my final project in my Data 101 course. </p>

<h1>
<a id="the-competition" class="anchor" href="#the-competition" aria-hidden="true"><span class="octicon octicon-link"></span></a>The Competition.</h1>

<p>The Competition</p>

<p>“Dialogues on social media can provide tremendous insight into the behaviors, desires, pains, and thoughts of consumers. We'd like your help in developing a repeatable process that identifies, classifies, and extracts the underlying drivers of consumer financial conversations and comments in social media data.”</p>

<p>Using just <a href="https://d18qs7yq39787j.cloudfront.net/uploads/contestfile/93/8af8575b213c-2015%2BWells%2BFargo%2BCampus%2BAnalytic%2BChallenge%2BDataset.txt">this document</a>, a team of 3 others and myself were tasked with answering two questions: </p>

<p>Question #1: What financial topics do consumers discuss on social media and what caused the consumers to post about this topic? </p>

<p>Question #2: Are the topics and “substance” consistent across the industry or are they isolated to individual banks? </p>

<p>With these two questions in mind, we had our work cut out for us.</p>

<h1>
<a id="the-solution" class="anchor" href="#the-solution" aria-hidden="true"><span class="octicon octicon-link"></span></a>The Solution</h1>

<p>Using the dataset, the first challenge – like in much of big data – was to structure or clean its contents. Un-cleaned, the social media posts looked like this:</p>

<p>As you can see there are many of the same items that pop up post after post. Some of them being “twit_hndl”, “BankD”, “INTERNET”, “Name”, etc. </p>

<p>Using a sample size of 10,000 posts, we stripped out words or topics (such as “twit_hndl”, “INTERNET”) that we believed would not lend any importance to subsequent analysis. </p>

<p>In an effort to clean, the data frame of 10,000 posts must be made suitable for the text mining package and therefore it must be a Corpus(collection of text documents). Here is the code to convert Data Frame to Corpus and to clean Corpus of unnecessary meta data.
'''R
library(tm) 
docs &lt;- Corpus(DataframeSource(as.data.frame(df[,6])))</p>

<h1>
<a id="remember-it-matters-the-order-in-which-tm_map-expressions-are-run" class="anchor" href="#remember-it-matters-the-order-in-which-tm_map-expressions-are-run" aria-hidden="true"><span class="octicon octicon-link"></span></a>REMEMBER: IT MATTERS THE ORDER IN WHICH TM_MAP EXPRESSIONS ARE RUN</h1>

<p>library(rJava)
docs &lt;- tm_map(docs, content_transformer(tolower)) # convert to lowercase first
myMeta &lt;- c("name","bank","banka","bankb","bankc", "bankd", "banke",
            "internet","https", "twit_hndl", "twit_hndl_banka",
            "twit_hndl_bankb","twit_hndl_bankc","twit_hndl_bankd", "phone",
            "dirmsg", "street","name_resp","bankds", "and", "for", "the", "you",
            "twithndlbanka","dirmsg","ret_twit")
docs &lt;- tm_map(docs, removeWords, myMeta)
docs &lt;- tm_map(docs, removeWords, stopwords('english'))
docs &lt;- tm_map(docs, removeWords, stopwords(kind = "SMART"))</p>

<h1>
<a id="metadatarecurrent-words" class="anchor" href="#metadatarecurrent-words" aria-hidden="true"><span class="octicon octicon-link"></span></a>metaData/recurrent words</h1>

<p>docs &lt;- tm_map(docs, stripWhitespace)
docs &lt;- tm_map(docs, removePunctuation)
docs &lt;- tm_map(docs, PlainTextDocument)
-&gt; '''</p>

<p>Following this code, the posts now look like the following</p>

<p>Now that the data has been cleaned, analysis can follow.</p>

<p>To find out what topics consumers talked about on social media, we went ahead and found the most frequent terms using 10,000 posts.</p>

<p>The Corpus must first be turned into a Document-Term Matrix
'''R</p>

<p>dtm &lt;- DocumentTermMatrix(docs)
dtm = removeSparseTerms(dtm, 0.98) #removes words with very low frequency
findFreqTerms(dtm,100) #finds words that occur at least 100x
-&gt; '''</p>

<h3>
<a id="creating-pages-manually" class="anchor" href="#creating-pages-manually" aria-hidden="true"><span class="octicon octicon-link"></span></a>Creating pages manually</h3>

<p>If you prefer to not use the automatic generator, push a branch named <code>gh-pages</code> to your repository to create a page manually. In addition to supporting regular HTML content, GitHub Pages support Jekyll, a simple, blog aware static site generator. Jekyll makes it easy to create site-wide headers and footers without having to copy them across every page. It also offers intelligent blog support and other advanced templating features.</p>

<h3>
<a id="authors-and-contributors" class="anchor" href="#authors-and-contributors" aria-hidden="true"><span class="octicon octicon-link"></span></a>Authors and Contributors</h3>

<p>You can <a href="https://github.com/blog/821" class="user-mention">@mention</a> a GitHub username to generate a link to their profile. The resulting <code>&lt;a&gt;</code> element will link to the contributor’s GitHub Profile. For example: In 2007, Chris Wanstrath (<a href="https://github.com/defunkt" class="user-mention">@defunkt</a>), PJ Hyett (<a href="https://github.com/pjhyett" class="user-mention">@pjhyett</a>), and Tom Preston-Werner (<a href="https://github.com/mojombo" class="user-mention">@mojombo</a>) founded GitHub.</p>

<h3>
<a id="support-or-contact" class="anchor" href="#support-or-contact" aria-hidden="true"><span class="octicon octicon-link"></span></a>Support or Contact</h3>

<p>Having trouble with Pages? Check out our <a href="https://help.github.com/pages">documentation</a> or <a href="https://github.com/contact">contact support</a> and we’ll help you sort it out.</p>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/giarroccoaj/wells-fargo-analytics">Wells Fargo Analytics Competition</a> is maintained by <a href="https://github.com/giarroccoaj">giarroccoaj</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>
